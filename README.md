# Different-Hand-Gesture-Recognition-Using-CNN

Introduction:
The necessity of Hand Gesture Recognition is having a well-defined dataset which have sufficient  number of images for every hand gesture by which we can predict the hand gesture more efficiently. Hand Gesture Recognition idea begin with the invention of glove-based control interface for the computer control, then over many years this technology was upgraded and now we are able to have a smart touchless hand gesture recognition product which reduce human efforts indefinitely.
For Real-time problems like:
i) Controlling a laptop/personal mobile using hand gesture recognition technique.
ii) For the people who are deaf or dumb understanding the talk using hand gesture recognition technique.
For the controlling a laptop/personal mobile we had to make some fix hand notations by using which the user can perform that specific task, and by using notations which are the user make we can use CNN which can help us to predict the notation user wants to convey thereby for that we can have a dataset for the notations which can help our model to predict the notation more accurately, hence the computer/mobile can do the right operation which user wants. As, 3D-CNN can be useful for detecting a set of action (i.e., the action which require hand moments) which can make the system more efficient and hence provide us with many new gesture notations. For people who are deaf or can’t speak the hand gesture recognition can help others to understand their talk with hands and them to understand the other people’s talk using hands. For this we can have some common actions which a person uses by expressing their feelings and based on that we can create some notations by which a general talk of normal people can also be understood by using this technique. This will let the disabled people to live a life like a normal human by understanding other’s and making the other’s understanding them. As, we get the notations for the following cases we can create a dataset by including all the types of images a user can use to express a specific notation, then we can train the dataset using CNN or 3D CNN as in general a notation may have more than one actions so by using it, we can predict the notations and its meaning based on the user actions accurately. Hand gestures are powerful human to human communication channel which convey a major part of information transfer in our everyday life. Hand gesture recognition is a process of understanding and classifying meaningful movements by the human hands. We need a well defined dataset is the necessity which has images for hand gestures by which we can predict the hand gestures more efficiently

The history of hand gesture recognition for computer control started with the invention of glovebased control interfaces. Researchers realized that gestures inspired by sign language can be used to offer simple commands for a computer interface.
Real Time Problems:-
i) In the automotive industry, this capability allows drivers and passengers to interact with the vehicle — usually to control the infotainment system without touching any buttons or screens.
ii) Sign Language Interpreter for disabled people
  i) A gesture recognition system starts with a camera pointed at a specific threedimensional zone within the vehicle, capturing frame-by-frame images of hand positions and motions. This camera is typically mounted in the roof module or other vantage point that is unlikely to be obstructed. The system illuminates the area with infrared LEDs or lasers for a clear image even when there is not much natural light. Those images are analysed in real time by computer vision and machine learning technologies, which translate the hand motions into commands, based on a predetermined library of signs.
  ii) How does sign language recognition work?
  
Identification of sign gesture is mainly performed by the following methods:
1. Glove-based method in which the signer has to wear a hardware glove, while the hand movements are getting captured.
2. Vision-based method, further classified into static and dynamic recognition.

Dataset Specification:
We are going to use a dataset which contains images of many different types of gesture (namely one finger notation gesture, two finger notation gesture, left notation gesture, etc.) and around 1000 images for each gesture it is the Hand Gesture Recognition Training Dataset. It contains images of gestures at different angles and clarity which helps our model to predict the hand gesture even in less clarity.

Different hand gestures:

![image](https://user-images.githubusercontent.com/78720027/217811805-94edff47-b089-43c0-a820-6d4fd3da3a05.png)

Results:

![image](https://user-images.githubusercontent.com/78720027/217812825-400c07f2-66b3-426e-927c-5d3d6f6ea981.png)

![image](https://user-images.githubusercontent.com/78720027/217812849-df55038c-ef46-4f4e-9941-6508801acbf5.png)

![image](https://user-images.githubusercontent.com/78720027/217812866-f50c9fe1-4963-40e5-abad-16f3b241bf6c.png)

![image](https://user-images.githubusercontent.com/78720027/217812898-e597657a-11b2-4ff5-817a-a998ba82f6e5.png)

![image](https://user-images.githubusercontent.com/78720027/217812909-7886330c-2f14-4de9-8150-fa004cb2e013.png)

![image](https://user-images.githubusercontent.com/78720027/217812917-7407080f-2c05-44b8-a9dc-6449160bcd00.png)

![image](https://user-images.githubusercontent.com/78720027/217812923-1f1f805b-4731-481d-9199-488aa94e7f8a.png)
